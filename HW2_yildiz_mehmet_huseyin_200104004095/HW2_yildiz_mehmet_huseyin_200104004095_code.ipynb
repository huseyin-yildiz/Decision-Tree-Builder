{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jry2J4qlMfQa",
        "JMh7ov8zMlJv",
        "UAejGuxViq3n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Downloading"
      ],
      "metadata": {
        "id": "75puqWmniiMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00475/audit_data.zip\n",
        "!unzip audit_data.zip\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
        "!unzip Bike-Sharing-Dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWRXvC0mrMxJ",
        "outputId": "dd1637e7-29e9-4c3a-c5db-63c1825fff08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 06:07:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/00475/audit_data.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28447 (28K) [application/x-httpd-php]\n",
            "Saving to: ‘audit_data.zip’\n",
            "\n",
            "audit_data.zip      100%[===================>]  27.78K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-06 06:07:38 (214 KB/s) - ‘audit_data.zip’ saved [28447/28447]\n",
            "\n",
            "Archive:  audit_data.zip\n",
            "  inflating: audit_data/audit_risk.csv  \n",
            "  inflating: audit_data/trial.csv    \n",
            "--2023-05-06 06:07:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279992 (273K) [application/x-httpd-php]\n",
            "Saving to: ‘Bike-Sharing-Dataset.zip’\n",
            "\n",
            "Bike-Sharing-Datase 100%[===================>] 273.43K   527KB/s    in 0.5s    \n",
            "\n",
            "2023-05-06 06:07:39 (527 KB/s) - ‘Bike-Sharing-Dataset.zip’ saved [279992/279992]\n",
            "\n",
            "Archive:  Bike-Sharing-Dataset.zip\n",
            "  inflating: Readme.txt              \n",
            "  inflating: day.csv                 \n",
            "  inflating: hour.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 (DT Classifier)"
      ],
      "metadata": {
        "id": "4Znki8gDivgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation ...:"
      ],
      "metadata": {
        "id": "jry2J4qlMfQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract the midpoints from given feature column\n",
        "def get_midpoints(feature_data):\n",
        "    # Sort the data in ascending order\n",
        "    sorted_data = sorted(feature_data)\n",
        "    # Remove duplicates\n",
        "    sorted_data = np.unique(sorted_data)\n",
        "    \n",
        "    # Find midpoints between adjacent values\n",
        "    midpoints = []\n",
        "    for i in range(len(sorted_data) - 1):\n",
        "        midpoint = (sorted_data[i] + sorted_data[i+1]) / 2.0\n",
        "        midpoints.append( midpoint )\n",
        "        \n",
        "    return midpoints\n",
        "\n",
        "\n",
        "# Class for decision nodes \n",
        "class Node:\n",
        "    \n",
        "    # Init function \n",
        "    def __init__(self, x, y, attribute_types):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.attribute_types = attribute_types\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.class_label = None     # If the node is leaf node\n",
        "\n",
        "        self.decision_value = None\n",
        "        self.decision_type = None\n",
        "        self.decision_attribute = None\n",
        "\n",
        "    # Calculate the entropy of the node with their data\n",
        "    def calc_entropy(self):\n",
        "        entropy = 0\n",
        "        classes = np.unique(self.y)\n",
        "        for cls in classes:\n",
        "            probab = np.count_nonzero(self.y == cls) / self.y.size\n",
        "            entropy -= probab * math.log2(probab)\n",
        "        return entropy\n",
        "\n",
        "    # Find all split points on the data\n",
        "    def find_split_pts(self):\n",
        "        split_pts = list()\n",
        "        \n",
        "        # Traverse all attributes\n",
        "        for (i,attribute) in enumerate(self.attribute_types):\n",
        "            \n",
        "            # Numeric   \n",
        "            if(attribute == 1):               \n",
        "                mid_pts = get_midpoints(self.x[:,i])\n",
        "                split_pts += [(i,x) for x in mid_pts]\n",
        "\n",
        "            # Categoric\n",
        "            elif(attribute == 2):             \n",
        "                uniques = np.unique(self.x[:,i])\n",
        "                \n",
        "                # Binary class --> just add one of them\n",
        "                if(len(uniques) == 2):\n",
        "                    split_pts.append( (i, uniques[0]) )\n",
        "                \n",
        "                # Multiclass --> add each one\n",
        "                elif(len(uniques) > 2):\n",
        "                    split_pts +=  [ (i,x) for x in uniques ]\n",
        "                \n",
        "                # If It has only one unique class for this data then do not add it to split points\n",
        "\n",
        "            # Invalid attribute type\n",
        "            else:\n",
        "                raise TypeError(\"Attribute type is not valid. Only 1 and 2 is valid \")\n",
        "\n",
        "        return  split_pts\n",
        "\n",
        "\n",
        "    # Generate left and right nodes with given split point\n",
        "    def generate_nodes(self, split_pt):\n",
        "        decision_type = self.attribute_types[split_pt[0]]\n",
        "        decision_value = split_pt[1]\n",
        "        \n",
        "        # Numeric\n",
        "        if(decision_type == 1):\n",
        "            mask = self.x[:,split_pt[0]] < decision_value\n",
        "            \n",
        "        # Categoric\n",
        "        elif(decision_type == 2):\n",
        "            mask = self.x[:,split_pt[0]] == decision_value\n",
        "        \n",
        "        # Invalid attribute type\n",
        "        else:\n",
        "            raise TypeError(\"Attribute type is not valid. Only 1 and 2 is valid \")\n",
        "\n",
        "        left = Node(self.x[mask], self.y[mask], self.attribute_types)\n",
        "        right = Node(self.x[~mask], self.y[~mask], self.attribute_types)\n",
        "\n",
        "        return (left, right)\n",
        "    \n",
        "\n",
        "    # Calculate the decision score by using left and right nodes. \n",
        "    # Explanation: Calculate the entropies of both left and right nodes and normalize them. \n",
        "    def calc_decision_score(self, node_left:'Node', node_right:'Node'):\n",
        "        \n",
        "        # Sizes of nodes\n",
        "        left_size = float( len(node_left.x) )\n",
        "        right_size = float( len(node_right.x) )\n",
        "        total = left_size + right_size\n",
        "\n",
        "        # Normalization\n",
        "        score = left_size / total * node_left.calc_entropy() + right_size / total * node_right.calc_entropy()\n",
        "\n",
        "        return score\n",
        "\n",
        "\n",
        "    # Predict the given data\n",
        "    # Go until leaf node recursively and return class label\n",
        "    def predict(self, data):\n",
        "        \n",
        "        if(self.class_label != None):\n",
        "            return self.class_label\n",
        "\n",
        "        # Numeric\n",
        "        if(self.decision_type == 1):\n",
        "           if( data[self.decision_attribute] < self.decision_value):\n",
        "               return self.left.predict(data)\n",
        "           \n",
        "           else:\n",
        "               return self.right.predict(data)    \n",
        "        \n",
        "        # Categoric\n",
        "        elif(self.decision_type == 2):\n",
        "            if( data[self.decision_attribute] == self.decision_value):\n",
        "               return self.left.predict(data)\n",
        "           \n",
        "            else:\n",
        "                return self.right.predict(data)    \n",
        "        \n",
        "\n",
        "        # Invalid attribute type\n",
        "        else:\n",
        "            raise TypeError(\"Attribute type is not valid. Only 1 and 2 is valid \")\n",
        "\n",
        "\n",
        "\n",
        "# Recursive and entropy based DT generation algorithm  by using greedy algorithm\n",
        "# Take data included root node and generate the DT recursively\n",
        "def generate_tree(node:Node , max_depth):\n",
        "\n",
        "    # If max depth is reached, then label the leaf node and terminates\n",
        "    if(max_depth <= 0):\n",
        "        counts = np.bincount(node.y)\n",
        "        most_freq = np.argmax(counts)\n",
        "        node.class_label = node.y[most_freq]\n",
        "        return\n",
        "    \n",
        "    # If the entropy of the current node is 0 then no need to continue anymore\n",
        "    # Label the leaf node and return\n",
        "    if(node.calc_entropy() == 0):\n",
        "        node.class_label = node.y[0]\n",
        "        return\n",
        "    \n",
        "    # Find all split points\n",
        "    split_points = node.find_split_pts()\n",
        "\n",
        "    # Variables to hold best split\n",
        "    best_split = None\n",
        "    best_score = 1.1\n",
        "    nodes = None\n",
        "\n",
        "    # Calculate scores of all split points and get the best split\n",
        "    for split_pt in split_points:\n",
        "        \n",
        "        # Generate the child nodes with split\n",
        "        node_left, node_right = node.generate_nodes(split_pt)\n",
        "        \n",
        "        # Calculate score\n",
        "        score = node.calc_decision_score(node_left, node_right)\n",
        "        \n",
        "        if( score < best_score ):\n",
        "            best_score = score\n",
        "            best_split = split_pt\n",
        "            nodes = (node_left, node_right)\n",
        "\n",
        "    \n",
        "    # End of For : Best split found.\n",
        "\n",
        "    # Place the children to the left and right \n",
        "    node.left, node.right = nodes[0],nodes[1]\n",
        "    \n",
        "    # Place the decision value and data type on the node\n",
        "    node.decision_value = best_split[1]\n",
        "    node.decision_type = node.attribute_types[best_split[0]]\n",
        "    node.decision_attribute = best_split[0]\n",
        "\n",
        "    # Recursive call for children\n",
        "    generate_tree(node.left, max_depth-1)\n",
        "    generate_tree(node.right, max_depth-1)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# DT builder\n",
        "def buid_dt(X, y, attribute_types, max_depth):\n",
        "    root = Node(X,y,attribute_types)\n",
        "    generate_tree(root, max_depth)\n",
        "    return root\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Takes DT and X matrix returns a vector for predicted predicted labels\n",
        "def predict_dt(dt:Node, X):\n",
        "    predict_vector = [dt.predict(x) for x in X]\n",
        "    return np.array( predict_vector )\n",
        "\n"
      ],
      "metadata": {
        "id": "ypHuGwnHNURg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv(\"/content/audit_data/trial.csv\")\n",
        "\n",
        "# Handling missing and NaN values\n",
        "# The values that can not cast to number will be NaN\n",
        "df1[\"LOCATION_ID\"] = pd.to_numeric( df1[\"LOCATION_ID\"], errors='coerce' ) \n",
        "df1[\"LOCATION_ID\"].isna().sum()\n",
        "df1 = df1.dropna()\n",
        "\n",
        "# Split x and y\n",
        "Y = df1[\"Risk\"].values\n",
        "X = df1.drop(\"Risk\",axis=1).values\n",
        "\n",
        "attribute_types = [2,2,1,2,1,2,1,2,2,1,2,2,2,2,2,2,2]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "k_fold = KFold(n_splits=6, shuffle=True, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "tGYvpMNZN085"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results:"
      ],
      "metadata": {
        "id": "lwQblSXBMkyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k, (train, test) in enumerate(k_fold.split(X, Y)):\n",
        "  # Train\n",
        "  dt = buid_dt(X[train], Y[train],attribute_types,5)\n",
        "  y_pred = predict_dt(dt, X[test])\n",
        "  print(\"\\n\\nFold\",k,\":\")\n",
        "  # Confusion matrix\n",
        "  conf_mat = confusion_matrix(Y[test], y_pred)\n",
        "  # Display confusion matrix\n",
        "  cm_df = pd.DataFrame(conf_mat, columns=['Predicted 0', 'Predicted 1'], index=['True 0', 'True 1'])\n",
        "  print('Confusion matrix:')\n",
        "  print(cm_df)\n",
        "  print(\"\\nResult:\")\n",
        "  print( classification_report(Y[test],y_pred) )\n",
        "  print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR2xNOp6Nm7j",
        "outputId": "e6f2f78b-7dea-41ae-d479-7dee9a576407"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Fold 0 :\n",
            "Confusion matrix:\n",
            "        Predicted 0  Predicted 1\n",
            "True 0           41            0\n",
            "True 1            0           88\n",
            "\n",
            "Result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        41\n",
            "           1       1.00      1.00      1.00        88\n",
            "\n",
            "    accuracy                           1.00       129\n",
            "   macro avg       1.00      1.00      1.00       129\n",
            "weighted avg       1.00      1.00      1.00       129\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold 1 :\n",
            "Confusion matrix:\n",
            "        Predicted 0  Predicted 1\n",
            "True 0           49            0\n",
            "True 1            0           80\n",
            "\n",
            "Result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        49\n",
            "           1       1.00      1.00      1.00        80\n",
            "\n",
            "    accuracy                           1.00       129\n",
            "   macro avg       1.00      1.00      1.00       129\n",
            "weighted avg       1.00      1.00      1.00       129\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold 2 :\n",
            "Confusion matrix:\n",
            "        Predicted 0  Predicted 1\n",
            "True 0           52            0\n",
            "True 1            0           77\n",
            "\n",
            "Result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        52\n",
            "           1       1.00      1.00      1.00        77\n",
            "\n",
            "    accuracy                           1.00       129\n",
            "   macro avg       1.00      1.00      1.00       129\n",
            "weighted avg       1.00      1.00      1.00       129\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold 3 :\n",
            "Confusion matrix:\n",
            "        Predicted 0  Predicted 1\n",
            "True 0           45            0\n",
            "True 1            0           84\n",
            "\n",
            "Result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        45\n",
            "           1       1.00      1.00      1.00        84\n",
            "\n",
            "    accuracy                           1.00       129\n",
            "   macro avg       1.00      1.00      1.00       129\n",
            "weighted avg       1.00      1.00      1.00       129\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold 4 :\n",
            "Confusion matrix:\n",
            "        Predicted 0  Predicted 1\n",
            "True 0           50            0\n",
            "True 1            0           78\n",
            "\n",
            "Result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        50\n",
            "           1       1.00      1.00      1.00        78\n",
            "\n",
            "    accuracy                           1.00       128\n",
            "   macro avg       1.00      1.00      1.00       128\n",
            "weighted avg       1.00      1.00      1.00       128\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fold 5 :\n",
            "Confusion matrix:\n",
            "        Predicted 0  Predicted 1\n",
            "True 0           49            0\n",
            "True 1            0           79\n",
            "\n",
            "Result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        49\n",
            "           1       1.00      1.00      1.00        79\n",
            "\n",
            "    accuracy                           1.00       128\n",
            "   macro avg       1.00      1.00      1.00       128\n",
            "weighted avg       1.00      1.00      1.00       128\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments and discussion:"
      ],
      "metadata": {
        "id": "JMh7ov8zMlJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, I implemented the DT builder as mentioned in the class. All techniques and methods are same as told in the class. The main idea is finding middle points of all features and calculate the score by using left and right entropy and normalize them. Then we take the best split point and put left and right nodes. We do this operation until the node is fully pure or max depth is reached. This algorithm works recursively as shown in the class. My first aim here was learning. So i tried the codes to be understandable as much as possible. To ease understanding and implementation i used node class. It contains the data on the node and related member functions like calc_entropy, find_split_pts etc. You may follow the comment lines for better understandanding of code. \n",
        "Results were amazing for me. It's same with sci-kit DT library. "
      ],
      "metadata": {
        "id": "xDz2kJ-EhrwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 (DT Regressor)"
      ],
      "metadata": {
        "id": "wdJNuW5yi5lR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation ...:"
      ],
      "metadata": {
        "id": "UAejGuxViq3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# Extract the midpoints from given feature column\n",
        "def get_midpoints(feature_data):\n",
        "    # Sort the data in ascending order\n",
        "    sorted_data = sorted(feature_data)\n",
        "    # Remove duplicates\n",
        "    sorted_data = np.unique(sorted_data)\n",
        "    \n",
        "    # Find midpoints between adjacent values\n",
        "    midpoints = []\n",
        "    for i in range(len(sorted_data) - 1):\n",
        "        midpoint = (sorted_data[i] + sorted_data[i+1]) / 2.0\n",
        "        midpoints.append( midpoint )\n",
        "        \n",
        "    return midpoints\n",
        "\n",
        "\n",
        "# Class for decision nodes \n",
        "class Node:\n",
        "    \n",
        "    # Init function \n",
        "    def __init__(self, x, y, attribute_types):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.attribute_types = attribute_types\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.regress_value = None     # If the leaf node\n",
        "\n",
        "        self.decision_value = None\n",
        "        self.decision_type = None\n",
        "        self.decision_attribute = None\n",
        "\n",
        "    # Calculates the error on a node\n",
        "    def calc_error(self):\n",
        "        mean = np.mean(self.y)\n",
        "        sum_err = 0    \n",
        "        for r in self.y:\n",
        "            sum_err += (r - mean)**2\n",
        "        error = sum_err / len(self.y)\n",
        "        return error\n",
        "\n",
        "    # Find all split points on the data\n",
        "    def find_split_pts(self):\n",
        "        split_pts = list()\n",
        "        \n",
        "        # Traverse all attributes\n",
        "        for (i,attribute) in enumerate(self.attribute_types):\n",
        "            \n",
        "            # Numeric   \n",
        "            if(attribute == 1):               \n",
        "                mid_pts = get_midpoints(self.x[:,i])\n",
        "                split_pts += [(i,x) for x in mid_pts]\n",
        "\n",
        "            # Categoric\n",
        "            elif(attribute == 2):             \n",
        "                uniques = np.unique(self.x[:,i])\n",
        "                \n",
        "                # Binary class --> just add one of them\n",
        "                if(len(uniques) == 2):\n",
        "                    split_pts.append( (i, uniques[0]) )\n",
        "                \n",
        "                # Multiclass --> add each one\n",
        "                elif(len(uniques) > 2):\n",
        "                    split_pts +=  [ (i,x) for x in uniques ]\n",
        "                \n",
        "                # If It has only one unique class for this data then do not add it to split points\n",
        "\n",
        "            # Invalid attribute type\n",
        "            else:\n",
        "                raise TypeError(\"Attribute type is not valid. Only 1 and 2 is valid \")\n",
        "\n",
        "        return  split_pts\n",
        "\n",
        "\n",
        "    # Generate left and right nodes with given split point\n",
        "    def generate_nodes(self, split_pt):\n",
        "        decision_type = self.attribute_types[split_pt[0]]\n",
        "        decision_value = split_pt[1]\n",
        "        \n",
        "        # Numeric\n",
        "        if(decision_type == 1):\n",
        "            mask = self.x[:,split_pt[0]] < decision_value\n",
        "            \n",
        "        # Categoric\n",
        "        elif(decision_type == 2):\n",
        "            mask = self.x[:,split_pt[0]] == decision_value\n",
        "        \n",
        "        # Invalid attribute type\n",
        "        else:\n",
        "            raise TypeError(\"Attribute type is not valid. Only 1 and 2 is valid \")\n",
        "\n",
        "        \n",
        "        # if(len(self.x[mask]) == 0):\n",
        "        #     raise \"split error\"\n",
        "        # if(len(self.x[~mask]) == 0):\n",
        "        #     raise \"split error\"\n",
        "        \n",
        "\n",
        "        left = Node(self.x[mask], self.y[mask], self.attribute_types)\n",
        "        right = Node(self.x[~mask], self.y[~mask], self.attribute_types)\n",
        "\n",
        "        return (left, right)\n",
        "    \n",
        "\n",
        "    # Calculate the split error by using left and right nodes. \n",
        "    # Explanation: Calculate the error of both left and right nodes and normalize them. \n",
        "    def calc_split_error(self, node_left:'Node', node_right:'Node'):\n",
        "        \n",
        "        # Sizes of nodes\n",
        "        left_size = float( len(node_left.x) )\n",
        "        right_size = float( len(node_right.x) )\n",
        "        total = left_size + right_size\n",
        "\n",
        "        # Normalization\n",
        "        split_error = left_size / total * node_left.calc_error() + right_size / total * node_right.calc_error()\n",
        "\n",
        "        return split_error\n",
        "\n",
        "\n",
        "    # Predict the given data\n",
        "    # Go until leaf node recursively and return regress value\n",
        "    def predict(self, data):\n",
        "        \n",
        "        if(self.regress_value != None):\n",
        "            return self.regress_value\n",
        "\n",
        "        # Numeric\n",
        "        if(self.decision_type == 1):\n",
        "           if( data[self.decision_attribute] < self.decision_value):\n",
        "               return self.left.predict(data)\n",
        "           \n",
        "           else:\n",
        "               return self.right.predict(data)    \n",
        "        \n",
        "        # Categoric\n",
        "        elif(self.decision_type == 2):\n",
        "            if( data[self.decision_attribute] == self.decision_value):\n",
        "               return self.left.predict(data)\n",
        "           \n",
        "            else:\n",
        "                return self.right.predict(data)    \n",
        "        \n",
        "\n",
        "        # Invalid attribute type\n",
        "        else:\n",
        "            raise TypeError(\"Attribute type is not valid. Only 1 and 2 is valid \")\n",
        "\n",
        "\n",
        "\n",
        "# Recursive and entropy based DT generation algorithm  by using greedy algorithm\n",
        "# Take data included root node and generate the DT recursively\n",
        "# Arg \"error_limit\" : limits the DT with a error rate. If reach this error rate then terminate the node \n",
        "def generate_tree(node:Node , max_depth, error_limit):\n",
        "\n",
        "    # If max depth is reached, then label the leaf node with mean value and return\n",
        "    if(max_depth <= 0):\n",
        "        node.regress_value = np.mean(node.y)\n",
        "        return\n",
        "    \n",
        "    # If the error of the current node is 0 then no need to continue anymore\n",
        "    # Label the leaf node with mean value and return\n",
        "    if(node.calc_error() <= error_limit):\n",
        "        node.regress_value = np.mean(node.y)\n",
        "        return\n",
        "    \n",
        "    # Find all split points\n",
        "    split_points = node.find_split_pts()\n",
        "\n",
        "    # Variables to hold best split\n",
        "    best_split = None\n",
        "    least_error = float('inf')\n",
        "    nodes = None\n",
        "\n",
        "    # Calculate errors of all split points and get the best split\n",
        "    for split_pt in split_points:\n",
        "        \n",
        "        # Generate the child nodes with split\n",
        "        node_left, node_right = node.generate_nodes(split_pt)\n",
        "        \n",
        "        # Calculate error\n",
        "        error = node.calc_split_error(node_left, node_right)\n",
        "        \n",
        "        if( error < least_error ):\n",
        "            least_error = error\n",
        "            best_split = split_pt\n",
        "            nodes = (node_left, node_right)\n",
        "\n",
        "    \n",
        "    # End of For : Best split found.\n",
        "\n",
        "    # Place the children to the left and right \n",
        "    node.left, node.right = nodes[0], nodes[1]\n",
        "    \n",
        "    # Place the decision value and data type on the node\n",
        "    node.decision_value = best_split[1]\n",
        "    node.decision_type = node.attribute_types[best_split[0]]\n",
        "    node.decision_attribute = best_split[0]\n",
        "\n",
        "    # Recursive call for children\n",
        "    generate_tree(node.left, max_depth-1, error_limit)\n",
        "    generate_tree(node.right, max_depth-1, error_limit)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# DT regressor builder\n",
        "def buid_rdf(X, y, attribute_types, N, error_limit):\n",
        "    root = Node(X,y,attribute_types)\n",
        "    generate_tree(root, N, error_limit)\n",
        "    return root\n",
        "\n",
        "\n",
        "\n",
        "# Takes DT and X matrix returns a vector for predicted predicted labels\n",
        "def predict_rdf(dt:Node, X):\n",
        "    predict_vector = [dt.predict(x) for x in X]\n",
        "    return np.array( predict_vector )\n"
      ],
      "metadata": {
        "id": "DE863chnisD3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df1 = pd.read_csv(\"day.csv\")\n",
        "\n",
        "\n",
        "# Split x and y\n",
        "Y = df1[\"cnt\"].values\n",
        "X = df1.drop(\"cnt\",axis=1)\n",
        "X = X.drop(\"dteday\",axis=1)\n",
        "X = X.drop(\"instant\",axis=1)\n",
        "\n",
        "X = X.values\n",
        "\n",
        "attribute_types = [2,2,2,2,2,2,2,1,1,1,1,1,1]\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, r2_score\n",
        "\n",
        "# K fold\n",
        "k_fold = KFold(n_splits=6, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "yddQi276kb9U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results:"
      ],
      "metadata": {
        "id": "Wjj3pygTjKTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean for R^2 score\n",
        "mean = 0    \n",
        "\n",
        "for k, (train, test) in enumerate(k_fold.split(X, Y)):\n",
        "  print(\"\\n\\nFold\",k,\"Result:\")\n",
        "  # Train\n",
        "  dt = buid_rdf(X[train], Y[train], attribute_types, N=5, error_limit=0)\n",
        "  y_pred = predict_rdf(dt, X[test])\n",
        "  \n",
        "  # Evaluate the model using R^2 score\n",
        "  r2 = r2_score(Y[test], y_pred)\n",
        "  print(\"R^2 score: {:.2f}\".format(r2))\n",
        "  mean += r2\n",
        "\n",
        "mean = mean / 6\n",
        "print(\"\\nMean R^2 score:\", mean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQgrEZRrjKvJ",
        "outputId": "198fbb16-8d97-4fe2-8a3f-1b0d9e90c4fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Fold 0 Result:\n",
            "R^2 score: 0.98\n",
            "\n",
            "\n",
            "Fold 1 Result:\n",
            "R^2 score: 0.97\n",
            "\n",
            "\n",
            "Fold 2 Result:\n",
            "R^2 score: 0.97\n",
            "\n",
            "\n",
            "Fold 3 Result:\n",
            "R^2 score: 0.98\n",
            "\n",
            "\n",
            "Fold 4 Result:\n",
            "R^2 score: 0.97\n",
            "\n",
            "\n",
            "Fold 5 Result:\n",
            "R^2 score: 0.97\n",
            "\n",
            "Mean R^2 score: 0.9715394786157862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments and discussion:"
      ],
      "metadata": {
        "id": "CSNot4HrjOqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this part, the basic idea of generating DT is same. There are some differences like calc_error calc_split_error etc. For example, Instead of calc_entropy function i use calc_error function, it calculates average error as told in the class. And also there are some little changes in the generate_tree function. It uses one more parameter error_limit. This parameter can be used ignoring some error while training. For example if the error_limit is setted to 5, then if a node is less than 5. The node will not continue to generate any more child nodes. This will decrease the overfitting and increase the run time performance. When it reaches leaf nodes it labels the regress value to the leaf node. While predicting a value it goes recursively until reaching a leaf node.\n",
        "\n"
      ],
      "metadata": {
        "id": "DV71IiFhk6if"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I wrote comment lines. You may follow them for better understanding of the codes. I implemented all formulas, algorithms in the lectures. I tried to do best. Thanks"
      ],
      "metadata": {
        "id": "yGFLwMMen3DL"
      }
    }
  ]
}